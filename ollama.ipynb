{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DevSavvy: Run an Ollama Server on Google Colab & Expose via ngrok üöÄ"
      ],
      "metadata": {
        "id": "o6fSw-ukxNYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Ollama ‚úÖ\n",
        "### Ollama provides a simple API for managing LLMs."
      ],
      "metadata": {
        "id": "OU8jOf3b3gtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "yhcu3b80xZtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install & Setup ngrok ‚úÖ\n",
        "### Download and extract the latest ngrok binary"
      ],
      "metadata": {
        "id": "jYX_Z_e8xdzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok-v3-stable-linux-amd64.tgz ngrok"
      ],
      "metadata": {
        "id": "7ZtwNLG7xh_Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authenticate ngrok\n",
        "\n",
        "* a ngrok auth token must be aquired in order to use it.\n",
        "\n",
        "* Follow the instruction below:\n",
        "https://ngrok.com/docs/getting-started/\n",
        "\n",
        "* Then replace your AuthToken with <NGROK_AUTH_TOKEN> below:\n"
      ],
      "metadata": {
        "id": "VHHPgjCVxlAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Ambil token dari secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('ngrok')\n",
        "\n",
        "# Pakai token ini untuk setup ngrok\n",
        "!./ngrok authtoken {NGROK_AUTH_TOKEN}\n"
      ],
      "metadata": {
        "id": "6DXYK77W62jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 3: Run Ollama & Expose API via ngrok üöÄ\n",
        "### Start the Ollama server in the background"
      ],
      "metadata": {
        "id": "Q_xIT3mWxpxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Start Ollama Serve"
      ],
      "metadata": {
        "id": "EUNi5gv47T8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Simpan ke variabel agar bisa dihentikan nanti\n",
        "ollama_proc = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "print(f\"üü¢ ollama serve dijalankan | PID: {ollama_proc.pid}\")\n"
      ],
      "metadata": {
        "id": "DGaLncdF17br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Start ngrok Serve"
      ],
      "metadata": {
        "id": "HX2m2GbM7tRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# 1. Jalankan ngrok secara background\n",
        "ngrok_proc = subprocess.Popen([\n",
        "    \"./ngrok\", \"http\", \"11434\", \"--host-header=localhost:11434\"\n",
        "])\n",
        "print(f\"üü¢ ngrok dijalankan | PID: {ngrok_proc.pid}\")\n",
        "\n",
        "# 2. Tunggu ngrok aktif\n",
        "time.sleep(5)  # penting agar ngrok siap dulu\n",
        "\n",
        "# 3. Ambil link dari ngrok API lokal\n",
        "try:\n",
        "    res = requests.get(\"http://localhost:4040/api/tunnels\")\n",
        "    tunnels = res.json()['tunnels']\n",
        "    public_url = tunnels[0]['public_url']\n",
        "    print(f\"üîó Link publik ngrok: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Gagal mengambil URL ngrok:\", e)\n"
      ],
      "metadata": {
        "id": "X7w3SJUZ2BTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Start llama3.2"
      ],
      "metadata": {
        "id": "ks8gidL77zhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Jalankan model LLM lokal melalui Ollama\n",
        "llama_proc = subprocess.Popen([\"ollama\", \"run\", \"llama3.2\"])\n",
        "print(f\"üü¢ Model llama3.2 dijalankan | PID: {llama_proc.pid}\")\n"
      ],
      "metadata": {
        "id": "LTmEWXpV2EKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###chek serve"
      ],
      "metadata": {
        "id": "DtAZbOUZ79M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"üìã Daftar proses terkait yang sedang berjalan:\\n\")\n",
        "\n",
        "# Proses yang sedang aktif untuk 'ollama', 'ngrok', dan 'llama3'\n",
        "for keyword in [\"ollama\", \"ngrok\", \"llama3\"]:\n",
        "    print(f\"\\nüîç Proses aktif: {keyword}\")\n",
        "    result = subprocess.run([\"ps\", \"-ef\"], capture_output=True, text=True)\n",
        "    lines = result.stdout.splitlines()\n",
        "    matched = [line for line in lines if keyword in line]\n",
        "    if matched:\n",
        "        for line in matched:\n",
        "            print(line)\n",
        "    else:\n",
        "        print(f\"‚ùå Tidak ada proses '{keyword}' ditemukan.\")\n"
      ],
      "metadata": {
        "id": "krzAIkqZz4aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop serve ollama"
      ],
      "metadata": {
        "id": "MnlHe3PTxwWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    ollama_proc.terminate()\n",
        "    ollama_proc.wait()\n",
        "    print(\"üõë Proses 'ollama serve' dihentikan dan dibersihkan.\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Gagal menghentikan 'ollama serve' (mungkin belum dijalankan atau sudah mati).\")\n"
      ],
      "metadata": {
        "id": "aLKMOfIR04GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop serve ngrok"
      ],
      "metadata": {
        "id": "QoABir1c8kRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    ngrok_proc.terminate()\n",
        "    ngrok_proc.wait()\n",
        "    print(\"üõë ngrok dihentikan dan dibersihkan.\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Gagal menghentikan 'ngrok' (mungkin belum dijalankan).\")\n"
      ],
      "metadata": {
        "id": "YZGdom_h0-Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop serve llama3.2"
      ],
      "metadata": {
        "id": "vN3pADRG8m-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llama_proc.terminate()\n",
        "    llama_proc.wait()\n",
        "    print(\"üõë llama3.2 dihentikan dan dibersihkan.\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Gagal menghentikan 'ollama run llama3.2' (mungkin belum dijalankan).\")\n"
      ],
      "metadata": {
        "id": "kwlKAv_81GKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ 1. Install Semua Dependensi (WAJIB)"
      ],
      "metadata": {
        "id": "oEeZMtonxrvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wajib install semua modul\n",
        "!pip install -U llama-index langchain langchain-community sentence-transformers"
      ],
      "metadata": {
        "id": "zPQWxdFDtHO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ 2. Load Dokumen dari Google Drive (LlamaIndex)"
      ],
      "metadata": {
        "id": "xpPz5M7Ax-Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# Arahkan ke folder dokumen distrik\n",
        "folder_path = \"/content/drive/MyDrive/Distrik\"\n",
        "\n",
        "# Load semua file termasuk dari subfolder\n",
        "documents = SimpleDirectoryReader(input_dir=folder_path, recursive=True).load_data()\n",
        "\n",
        "# Tampilkan jumlah & nama file\n",
        "print(f\"\\n‚úÖ Total dokumen terbaca: {len(documents)}\\n\")\n",
        "\n",
        "for i, doc in enumerate(documents, start=1):\n",
        "    file_path = doc.metadata.get(\"file_path\", \"Tidak diketahui\")\n",
        "    print(f\"{i}. {file_path}\")\n"
      ],
      "metadata": {
        "id": "rTNLuWnAx4d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "\n",
        "# Inisialisasi embedding model lokal\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Buat konteks layanan dengan embedding lokal\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
        "\n",
        "# Bangun index dari dokumen yang sudah diload sebelumnya\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "print(\"‚úÖ Index selesai dibuat.\")\n"
      ],
      "metadata": {
        "id": "tv6nEO9czMIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}